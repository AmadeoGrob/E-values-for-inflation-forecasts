# ============================================================
# Inflation Forecasting - DRF Modeling Script (Direct Forecasting) - CORRECTED
# ============================================================
# This script loads prepared design matrices and performs direct, multi-horizon DRF modeling
# by fitting a separate model for each forecast horizon.
# Input: CSV files from data preparation script
# Output: DRF forecasts, variable importance, and visualizations
# ---- 0. Packages ----------------------------------------------------------
library(here)
library(readr)
library(dplyr)
library(tidyr)
library(drf)
library(ggplot2)
library(scoringRules)
library(lubridate)
# ---- 1. Configuration -----------------------------------------------------
CONFIG <- list(
# Paths
prepared_data_dir = file.path(here::here("..", ".."), "data", "processed", "for_drf"),
out_dir = file.path(here::here("..", ".."), "results", "forecasts", "drf"),
# DRF parameters
num_trees = 2000,
splitting_rule = "FourierMMD",
sample_fraction = 0.5,
min_node_size = 5,
# Forecast parameters
quantiles = c(0.025, 0.05, 0.10, 0.20, 0.25, 0.30, 0.40, 0.50, 0.60, 0.70, 0.75, 0.80, 0.90, 0.95, 0.975),
quantile_names = c("q025", "q05", "q10", "q20", "q25", "q30", "q40", "q50", "q60", "q70", "q75", "q80", "q90", "q95", "q975"),
# Plotting
vip_top_n = 25,
# Other
seed = 42
)
# ---- 2. Helper Functions --------------------------------------------------
#' Loads data with multiple target columns
load_design_matrix <- function(region_name, data_dir) {
design_matrix <- read_csv(file.path(data_dir, sprintf("%s_design_matrix.csv", region_name)), show_col_types = FALSE)
metadata <- read_csv(file.path(data_dir, sprintf("%s_metadata.csv", region_name)), show_col_types = FALSE)
target_cols <- grep("^target_h", names(design_matrix), value = TRUE)
feature_cols <- setdiff(names(design_matrix), c("date", target_cols))
X <- as.data.frame(design_matrix[, feature_cols])
Y_df <- as.data.frame(design_matrix[, target_cols])
dates <- as.Date(design_matrix$date)
# Horizons are read directly from metadata for consistency
horizons <- as.numeric(unlist(strsplit(metadata$horizons, ", ")))
cat(sprintf("Loaded %s: %d rows × %d features | Horizons: %s\n",
toupper(region_name), nrow(X), ncol(X), paste(horizons, collapse=", ")))
list(
X = X,
Y_df = Y_df,
dates = dates,
horizons = horizons,
metadata = metadata
)
}
#' Store distribution weights and values
store_dist <- function(w_row, y_train) {
nz <- which(w_row > 0)
list(idx = nz, w = w_row[nz], y = y_train[nz])
}
#' Fit DRF model with consistent parameters
fit_drf <- function(X, Y) {
set.seed(CONFIG$seed)
drf(
X = X, Y = Y,
num.trees = CONFIG$num_trees,
splitting.rule = CONFIG$splitting_rule,
sample.fraction = CONFIG$sample_fraction,
min.node.size = CONFIG$min_node_size
)
}
#' Calculate and plot variable importance per horizon
analyze_variable_importance <- function(X, Y_vec, region_name, horizon) {
set.seed(CONFIG$seed)
fit <- fit_drf(X, Y_vec)
vip <- suppressWarnings(variable_importance(fit))
names(vip) <- colnames(X)
top <- sort(vip, decreasing = TRUE)[1:CONFIG$vip_top_n]
p <- ggplot(data.frame(var = factor(names(top), levels = names(top)), imp = top),
aes(x = reorder(var, imp), y = imp)) +
geom_col() + coord_flip() +
labs(x = NULL, y = "MMD importance",
title = sprintf("Top %d Predictors (%s, h=%d)", CONFIG$vip_top_n, toupper(region_name), horizon)) +
theme_minimal(base_size = 11)
print(p)
}
#' Perform rolling-origin forecasting for a single region and horizon
rolling_forecast <- function(X_df, Y_vec, dates_vec, init_train_size, horizon) {
set.seed(CONFIG$seed)
n_tot <- nrow(X_df)
n_fore <- n_tot - init_train_size
pred_mat <- matrix(NA_real_, n_fore, length(CONFIG$quantiles), dimnames = list(NULL, CONFIG$quantile_names))
dist_list <- vector("list", n_fore)
mean_vec <- numeric(n_fore)
cutoff_vec <- as.Date(rep(NA, n_fore))
target_time_vec <- as.Date(rep(NA, n_fore))
for (i in seq_len(n_fore)) {
if (i %% 50 == 0) cat(sprintf("  h=%d progress: %d/%d\n", horizon, i, n_fore))
idx_tr <- 1:(init_train_size + i - 1)
idx_pred <- init_train_size + i
# The date of the features used to make the forecast
cutoff_vec[i] <- dates_vec[idx_pred]
# The date that the forecast is for
target_time_vec[i] <- dates_vec[idx_pred] %m+% months(horizon)
fit_i <- fit_drf(X_df[idx_tr, ], Y_vec[idx_tr])
raw <- predict(fit_i, newdata = X_df[idx_pred, , drop = FALSE])
w_i <- as.numeric(raw$weights[1, ])
mean_vec[i] <- sum(w_i * raw$y)
dist_list[[i]] <- store_dist(w_i, raw$y)
pred_mat[i, ] <- predict(fit_i, newdata = X_df[idx_pred, , drop = FALSE],
functional = "quantile", quantiles = CONFIG$quantiles)$quantile
}
results <- data.frame(
cutoff = cutoff_vec,
target_time = target_time_vec,
horizon_step = horizon,
y_true = Y_vec[(init_train_size + 1):n_tot]
)
results <- cbind(results, pred_mat, data.frame(mean_vec = mean_vec))
list(results = results, dist_list = dist_list)
}
#' Calculate and report key performance metrics
calculate_metrics <- function(results, dist_list) {
# Ensure no NA values in truth or predictions before calculating
valid_rows <- complete.cases(results)
if (sum(valid_rows) == 0) {
return(list(rmse = NA, mae = NA, crps = NA))
}
res_valid <- results[valid_rows, ]
dist_valid <- dist_list[valid_rows]
rmse <- sqrt(mean((res_valid$y_true - res_valid$mean_vec)^2))
mae <- mean(abs(res_valid$y_true - res_valid$q50))
crps_vec <- sapply(seq_len(nrow(res_valid)), function(i) {
d <- dist_valid[[i]]
crps_sample(y = res_valid$y_true[i], dat = d$y, w = d$w, method = "edf")
})
crps <- mean(crps_vec, na.rm = TRUE)
cat(sprintf("  Metrics -> RMSE: %.3f | MAE: %.3f | CRPS: %.3f\n", rmse, mae, crps))
list(rmse = rmse, mae = mae, crps = crps)
}
#' Save forecast distribution to RDS file
save_forecast_distributions <- function(dist_list, region_name, horizon, output_dir) {
if (!dir.exists(output_dir)) dir.create(output_dir, recursive = TRUE)
# Only save the distributions here
saveRDS(dist_list, file.path(output_dir, sprintf("%s_h%d_distributions.rds", region_name, horizon)))
cat(sprintf("  Saved distribution for %s (h=%d)\n", toupper(region_name), horizon))
}
# ---- 3. Main Modeling Pipeline --------------------------------------------
#' Run DRF analysis on prepared data
run_drf_analysis <- function(regions = c("ch", "eu", "us"), save_results = TRUE) {
if (!dir.exists(CONFIG$prepared_data_dir)) {
stop(sprintf("Prepared data directory not found. Run data prep script first."))
}
design_data_list <- list()
all_metrics <- list()
for (region in regions) {
tryCatch({
design_data_list[[region]] <- load_design_matrix(region, CONFIG$prepared_data_dir)
}, error = function(e) {
cat(sprintf("Error loading data for %s: %s\n", toupper(region), e$message))
})
}
if (length(design_data_list) == 0) stop("No valid data loaded.")
full_forecast_output <- list()
for (region in names(design_data_list)) {
cat(sprintf("\n--- Processing Region: %s ---\n", toupper(region)))
region_data <- design_data_list[[region]]
region_results_list <- list()
# List to collect results data frames FOR THIS REGION ONLY
region_results_df_list <- list()
# --- MAIN HORIZON LOOP ---
for (h in region_data$horizons) {
cat(sprintf("\n-- Forecasting for horizon h=%d --\n", h))
Y_h_full <- region_data$Y_df[[paste0("target_h", h)]]
last_valid_idx <- max(which(!is.na(Y_h_full)))
init_train_size <- region_data$metadata$init_train_size
if (last_valid_idx <= init_train_size) {
cat(sprintf("  Skipping h=%d: Not enough observations for a forecast.\n", h))
next
}
X_h <- region_data$X[1:last_valid_idx, ]
Y_h <- Y_h_full[1:last_valid_idx]
dates_h <- region_data$dates[1:last_valid_idx]
analyze_variable_importance(X_h, Y_h, region, h)
forecast_output_h <- rolling_forecast(
X_df = X_h, Y_vec = Y_h, dates_vec = dates_h,
init_train_size = init_train_size, horizon = h
)
metrics_h <- calculate_metrics(forecast_output_h$results, forecast_output_h$dist_list)
# Store metrics for final summary
metric_row <- data.frame(
Model        = paste0("DRF_", toupper(region)),
horizon_step = h,
RMSE         = metrics_h$rmse,
MAE          = metrics_h$mae,
CRPS         = metrics_h$crps
)
all_metrics <- append(all_metrics, list(metric_row))
forecast_output_h$metrics <- metrics_h
if (save_results) {
# Save RDS distribution file (one per horizon)
save_forecast_distributions(forecast_output_h$dist_list, region, h, CONFIG$out_dir)
# Collect the results data frame for this horizon
region_results_df_list <- append(region_results_df_list, list(forecast_output_h$results))
}
region_results_list[[paste0("h", h)]] <- forecast_output_h
}
# --- AGGREGATE AND SAVE FOR THE REGION ---
# After all horizons for a region are done, combine and save its CSV
if (save_results && length(region_results_df_list) > 0) {
combined_region_forecasts <- dplyr::bind_rows(region_results_df_list)
output_path <- file.path(CONFIG$out_dir, sprintf("%s_drf_forecast_results.csv", region))
write_csv(combined_region_forecasts, output_path)
cat(sprintf("\nSaved combined forecast results for %s to: %s\n", toupper(region), output_path))
}
full_forecast_output[[region]] <- region_results_list
}
# Combine all metrics and save to a single CSV
if (save_results && length(all_metrics) > 0) {
final_metrics_df <- dplyr::bind_rows(all_metrics)
metrics_output_path <- file.path(CONFIG$out_dir, "drf_evaluation_summary.csv")
write_csv(final_metrics_df, metrics_output_path)
cat(sprintf("\nSaved combined evaluation metrics to: %s\n", metrics_output_path))
}
if (save_results) cat("\nAll files saved to:", CONFIG$out_dir, "\n")
invisible(list(forecast_output = full_forecast_output, config_used = CONFIG))
}
# ---- 4. Execute DRF Analysis ----------------------------------------------
if (interactive()) {
results <- run_drf_analysis(save_results = TRUE)
# You can now access results like:
# results$forecast_output$ch$h1$results  # Data frame of results for CH, h=1
# results$forecast_output$us$h12$metrics # Metrics for US, h=12
}
# ============================================================
# Inflation Forecasting - Data Preparation Script
# ============================================================
# This script handles data loading, preprocessing, and creation of design matrices
# Output: Saves design matrices and metadata for DRF modeling
# ---- 0. Packages ----------------------------------------------------------
library(here)
library(readr)
library(lubridate)
library(dplyr)
library(tidyr)
library(zoo)
# ---- 1. Configuration -----------------------------------------------------
CONFIG <- list(
# Paths
root_dir = here::here("..", ".."),
data_dir = file.path(here::here("..", ".."), "data", "processed"),
prepared_data_dir = file.path(here::here("..", ".."), "data", "processed", "for_drf"),
# Feature engineering parameters
lag_max = 1,  # number of lagged regressors
target_lag_max = 11, # number of lags for the target variable
horizons = c(1, 2, 3, 6, 12),  # forecast horizons
include_current = TRUE, # Include current period (L0) in design matrix
# Training parameters
init_train_pct = 0.4,  # Use 40% of each dataset for initial training
min_train_obs = 100,
# Other
seed = 42
)
# ---- 2. Variable Sets -----------------------------------------------------
VARIABLE_SETS <- list(
ch = c(
"cpi_total_yoy", "cpi_goods_cat_goods_ind", "cpi_goods_cat_services_ind",
"cpi_housing_energy_ind", "cpi_food_nonalcoholic_beverages_ind",
"cpi_transport_ind", "cpi_health_ind", "cpi_clothing_footwear_ind",
"cpi_alcoholic_beverages_tobacco_ind",
"cpi_household_furniture_furnishings_routine_maintenance_ind",
"cpi_restaurants_hotels_ind", "cpi_recreation_culture_ind",
"cpi_communications_ind", "cpi_education_ind",
"mon_stat_mon_agg_m0_total_chf",
"ppi_total_base_month_december_2020_ind",
"ipi_total_base_month_december_2020_ind", "oilpricex"
),
eu = c(
"hcpi_yoy", "irt3m_eacc", "irt6m_eacc", "ltirt_eacc", "ppicag_ea",
"ppicog_ea", "ppindcog_ea", "ppidcog_ea", "ppiing_ea", "ppinrg_ea",
"hicpnef_ea", "hicpg_ea", "hicpin_ea", "hicpsv_ea", "hicpng_ea",
"curr_eacc", "m2_eacc", "m1_eacc", "oilpricex"
),
us = c(
"cpi_all_yoy", "m1sl", "m2sl", "m2real", "busloans", "fedfunds",
"tb3ms", "tb6ms", "gs1", "gs5", "gs10", "ppicmm",
"oilpricex", "cpiappsl", "cpitrnsl", "cpimedsl", "cusr0000sac",
"cusr0000sad", "cusr0000sas", "pcepi"
)
)
# ---- 3. Helper Functions --------------------------------------------------
#' Preprocess data into multivariate zoo object
preprocess_multivar <- function(df, columns) {
if (!"date" %in% names(df)) stop("Column 'date' is missing.")
df <- df %>%
mutate(date = as.Date(ymd(date)),
date = ceiling_date(date, "month") - days(1)) %>%
arrange(date)
# Warn about missing columns
missing_cols <- setdiff(columns, names(df))
if (length(missing_cols) > 0) {
warning("Missing columns: ", paste(missing_cols, collapse = ", "))
}
present_cols <- intersect(columns, names(df))
ts_df <- df %>%
select(date, all_of(present_cols))
# Crop data to start where target variable becomes available
# Search for explicit target variable names
target_names <- c("cpi_total_yoy", "hcpi_yoy", "cpi_all_yoy")
target_var <- intersect(target_names, names(ts_df))[1]  # Get first match
if (target_var %in% names(ts_df)) {
first_valid_idx <- which(!is.na(ts_df[[target_var]]))[1]
if (!is.na(first_valid_idx)) {
ts_df <- ts_df[first_valid_idx:nrow(ts_df), ]
cat(sprintf("Cropped to start where %s is available: %d rows from %s to %s\n",
target_var, nrow(ts_df), min(ts_df$date), max(ts_df$date)))
}
}
zoo(ts_df[, -1], order.by = ts_df$date)
}
make_xy <- function(x, y_name, lag_max = 5, horizons = c(1, 2, 3, 6, 12), include_current = TRUE, region_name = "unknown") {
stopifnot(y_name %in% colnames(x))
x <- x[order(index(x)), ]
n_obs <- nrow(x)
dates <- index(x)
# Calculate usable range
longest_lag <- max(lag_max, CONFIG$target_lag_max)
first_usable <- longest_lag + 1
# Base the number of rows on the shortest horizon to maximize data
last_usable <- n_obs - min(horizons)
n_usable <- last_usable - first_usable + 1
if (n_usable <= 0) {
stop(sprintf("Insufficient data: need at least %d observations", longest_lag + min(horizons) + 1))
}
# Create design matrix
X_list <- list()
var_names <- colnames(x)
# Current period (L0) - if we include current
if (include_current) {
for (j in 1:ncol(x)) {
X_list[[paste0(var_names[j], "_L0")]] <- as.numeric(x[first_usable:last_usable, j])
}
}
# Lagged periods (L1 to lag_max)
for (lag in 1:lag_max) {
for (j in 1:ncol(x)) {
X_list[[paste0(var_names[j], "_L", lag)]] <- as.numeric(x[(first_usable-lag):(last_usable-lag), j])
}
}
# Additional lags for target variable only (up to target_lag_max)
if (longest_lag > lag_max) {
for (lag in (lag_max + 1):longest_lag) {
X_list[[paste0(y_name, "_L", lag)]] <- as.numeric(x[(first_usable-lag):(last_usable-lag), y_name])
}
}
# Combine into data frame
X_df <- as.data.frame(X_list)
dates_vec <- dates[first_usable:last_usable]
# Add differenced features
for (lag in 1:lag_max) {
for (var in var_names) {
current_lag_col <- paste0(var, "_L", lag - 1)
previous_lag_col <- paste0(var, "_L", lag)
if (current_lag_col %in% names(X_df) && previous_lag_col %in% names(X_df)) {
X_df[[paste0(var, "_D", lag)]] <- X_df[[current_lag_col]] - X_df[[previous_lag_col]]
}
}
}
# Additional differenced periods for TARGET variable only
if (longest_lag > lag_max) {
for (lag in (lag_max + 1):longest_lag) {
current_lag_col <- paste0(y_name, "_L", lag - 1)
previous_lag_col <- paste0(y_name, "_L", lag)
if (current_lag_col %in% names(X_df) && previous_lag_col %in% names(X_df)) {
X_df[[paste0(y_name, "_D", lag)]] <- X_df[[current_lag_col]] - X_df[[previous_lag_col]]
}
}
}
# Create target columns (Y part) for multiple horizons
Y_df <- as.data.frame(lapply(horizons, function(h) {
y_vec <- rep(NA, n_usable)
# Determine how many future values are actually available in the original ts
num_valid_targets <- n_obs - (first_usable + h) + 1
# We can only fill up to the number of rows we have for X
num_to_fill <- min(n_usable, num_valid_targets)
if (num_to_fill > 0) {
# The start index in the original series 'x'
target_start_idx <- first_usable + h
y_vec[1:num_to_fill] <- as.numeric(x[target_start_idx:(target_start_idx + num_to_fill - 1), y_name])
}
return(y_vec)
}))
names(Y_df) <- paste0("target_h", horizons)
# Handle missing values in features (X) ONLY
if (sum(is.na(X_df)) > 0) {
complete_cases_X <- complete.cases(X_df)
X_df <- X_df[complete_cases_X, ]
Y_df <- Y_df[complete_cases_X, ]
dates_vec <- dates_vec[complete_cases_X, ]
}
# Calculate training size: 40% of original cleaned dataset minus longest_lag
original_n <- n_obs
init_train_size <- max(CONFIG$min_train_obs, round(original_n * CONFIG$init_train_pct) - longest_lag)
cat(sprintf("Created %d × %d design matrix (%d initial training (40%% of %d months - %d lags))\n",
nrow(X_df), ncol(X_df), init_train_size, original_n, longest_lag))
# Create complete design matrix
design_matrix <- data.frame(
date = dates_vec,
X_df,
Y_df
)
list(
design_matrix = design_matrix,
init_train_size = init_train_size,
target_var = y_name
)
}
#' Save design matrix and metadata to CSV files
save_design_matrix <- function(design_data, region_name, output_dir) {
if (!dir.exists(output_dir)) dir.create(output_dir, recursive = TRUE)
# Save complete design matrix
write_csv(design_data$design_matrix, file.path(output_dir, sprintf("%s_design_matrix.csv", region_name)))
# Save metadata
metadata <- data.frame(
region = region_name,
target_variable = design_data$target_var,
n_observations = nrow(design_data$design_matrix),
n_features = ncol(design_data$design_matrix) - 1 - length(CONFIG$horizons),  # Subtract date and all target columns
init_train_size = design_data$init_train_size,
date_start = min(design_data$design_matrix$date),
date_end = max(design_data$design_matrix$date),
lag_max = CONFIG$lag_max,
horizons = paste(CONFIG$horizons, collapse = ", "),
include_current = CONFIG$include_current
)
write_csv(metadata, file.path(output_dir, sprintf("%s_metadata.csv", region_name)))
cat(sprintf("Saved design matrix for %s to CSV files\n", toupper(region_name)))
list(
design_matrix_file = file.path(output_dir, sprintf("%s_design_matrix.csv", region_name)),
metadata_file = file.path(output_dir, sprintf("%s_metadata.csv", region_name))
)
}
# ---- 4. Main Data Preparation Pipeline ------------------------------------
#' Run data preparation and save design matrices
prepare_data <- function() {
# Create output directory
if (!dir.exists(CONFIG$prepared_data_dir)) {
dir.create(CONFIG$prepared_data_dir, recursive = TRUE)
}
# Load data
datasets <- list(
ch = read_csv(file.path(CONFIG$data_dir, "ch_data_final.csv"), show_col_types = FALSE),
eu = read_csv(file.path(CONFIG$data_dir, "eu_data_final.csv"), show_col_types = FALSE),
us = read_csv(file.path(CONFIG$data_dir, "us_data_final.csv"), show_col_types = FALSE)
)
# Process each region
saved_files <- list()
for (region in names(VARIABLE_SETS)) {
# Preprocess to multivariate time series
ts_zoo <- preprocess_multivar(datasets[[region]], VARIABLE_SETS[[region]])
# Select target variable by name
target_names <- c("cpi_total_yoy", "hcpi_yoy", "cpi_all_yoy")
target_var <- intersect(target_names, names(ts_zoo))[1]
# Check if target variable was found
if (is.na(target_var)) {
cat(sprintf("Warning: No target variable found for %s. Available: %s\n",
toupper(region), paste(names(ts_zoo)[1:5], collapse = ", ")))
next
}
# Create design matrix
design_data <- make_xy(ts_zoo, target_var, CONFIG$lag_max, CONFIG$horizons,
CONFIG$include_current, region)
cat(sprintf("Design matrix: %d rows × %d features | Target: %s\n",
nrow(design_data$design_matrix),
ncol(design_data$design_matrix) - 1 - length(CONFIG$horizons),
target_var))
# Save to CSV files
saved_files[[region]] <- save_design_matrix(design_data, region, CONFIG$prepared_data_dir)
}
# Save overall configuration
config_df <- data.frame(
parameter = names(CONFIG),
value = sapply(CONFIG, function(x) ifelse(is.list(x), paste(x, collapse = ", "), as.character(x)))
)
write_csv(config_df, file.path(CONFIG$prepared_data_dir, "config.csv"))
cat(sprintf("\nData preparation complete! Files saved to: %s\n", CONFIG$prepared_data_dir))
cat("Files created per region:\n")
cat("  - {region}_design_matrix.csv: Complete design matrix (date, target_date, features, target)\n")
cat("  - {region}_metadata.csv: Dataset metadata\n")
cat("  - config.csv: Configuration parameters\n")
return(saved_files)
}
# ---- 5. Execute Data Preparation ------------------------------------------
# Run data preparation
files_saved <- prepare_data()
# ---- 6. Check final files ------------------------------------------------
for (region in names(files_saved)) {
file_path <- files_saved[[region]]$design_matrix_file
if (file.exists(file_path)) {
# Create a unique name for each dataframe, e.g., "ch_df"
new_var_name <- paste0(region, "_df")
# Read the CSV and assign it to the new variable name in the global environment
assign(
new_var_name,
read_csv(file_path, show_col_types = FALSE),
envir = .GlobalEnv
)
} else {
cat(sprintf("File not found for region %s: %s\n", toupper(region), file_path))
}
}
