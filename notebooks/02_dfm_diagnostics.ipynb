{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "703752b2",
   "metadata": {},
   "source": [
    "# Lag Selection for Dynamic Factor Model\n",
    "\n",
    "This notebook implements a systematic approach to select the optimal number of lags and factors for Dynamic Factor Models (DFM). The lag selection process is performed on rolling splits of the data for three regions: Switzerland (CH), the Euro Area (EU), and the United States (US)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195b0ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the root directory of the repo to sys.path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT_DIR = Path(\"../\").resolve()\n",
    "sys.path.append(str(ROOT_DIR))\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "from src.dfm_utils.helpers import preprocess_multivar, dynamic_factors, bai_ng_criteria, onatski_criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d1ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets for each region\n",
    "DATA_DIR = ROOT_DIR / \"data\" / \"processed\"\n",
    "datasets = {\n",
    "    \"ch\": pd.read_csv(DATA_DIR / \"ch_data_final.csv\"),\n",
    "    \"eu\": pd.read_csv(DATA_DIR / \"eu_data_final.csv\"),\n",
    "    \"us\": pd.read_csv(DATA_DIR / \"us_data_final.csv\"),\n",
    "}\n",
    "\n",
    "datasets_win = {\n",
    "    \"ch\": pd.read_csv(DATA_DIR / \"ch_data_transformed_win.csv\"),\n",
    "    \"eu\": pd.read_csv(DATA_DIR / \"eu_data_transformed_win.csv\"),\n",
    "    \"us\": pd.read_csv(DATA_DIR / \"us_data_transformed_win.csv\"),\n",
    "}\n",
    "\n",
    "# Define variable sets for each region (compact)\n",
    "variable_sets = {\n",
    "    \"ch\": [\n",
    "        \"cpi_total_yoy\", \"cpi_goods_cat_goods_ind\", \"cpi_goods_cat_services_ind\", \"cpi_housing_energy_ind\",\n",
    "        \"cpi_food_nonalcoholic_beverages_ind\", \"cpi_transport_ind\", \"cpi_health_ind\", \"cpi_clothing_footwear_ind\",\n",
    "        \"cpi_alcoholic_beverages_tobacco_ind\", \"cpi_household_furniture_furnishings_routine_maintenance_ind\",\n",
    "        \"cpi_restaurants_hotels_ind\", \"cpi_recreation_culture_ind\", \"cpi_communications_ind\", \"cpi_education_ind\",\n",
    "        \"mon_stat_mon_agg_m0_total_chf\", \"ppi_total_base_month_december_2020_ind\", \"ipi_total_base_month_december_2020_ind\",\n",
    "        \"oilpricex\"\n",
    "    ],\n",
    "    \"eu\": [\n",
    "        \"hcpi_yoy\", \"irt3m_eacc\", \"irt6m_eacc\", \"ltirt_eacc\", \"ppicag_ea\", \"ppicog_ea\", \"ppindcog_ea\", \"ppidcog_ea\",\n",
    "        \"ppiing_ea\", \"ppinrg_ea\", \"hicpnef_ea\", \"hicpg_ea\", \"hicpin_ea\", \"hicpsv_ea\", \"hicpng_ea\", \"curr_eacc\",\n",
    "        \"m2_eacc\", \"m1_eacc\", \"oilpricex\"\n",
    "    ],\n",
    "    \"us\": [\n",
    "        \"cpi_all_yoy\", \"m1sl\", \"m2sl\", \"m2real\", \"busloans\", \"fedfunds\", \"tb3ms\", \"tb6ms\", \"gs1\", \"gs5\", \"gs10\",\n",
    "        \"ppicmm\", \"oilpricex\", \"cpiappsl\", \"cpitrnsl\", \"cpimedsl\", \"cusr0000sac\", \"cusr0000sad\", \"cusr0000sas\", \"pcepi\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Define transformed variable sets for each region\n",
    "variable_sets_t = {\n",
    "    region: [f\"{var}_t\" for var in variables]\n",
    "    for region, variables in variable_sets.items()\n",
    "}\n",
    "\n",
    "# Load multivariate data\n",
    "multivar_data = {}\n",
    "for region, columns in variable_sets.items():\n",
    "    multivar_data[region] = preprocess_multivar(datasets[region], columns)\n",
    "    print(\n",
    "        f\"{region.upper()}: {multivar_data[region].shape} - columns: {list(multivar_data[region].columns)}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"{region.upper()} time series range: {multivar_data[region].index[0].strftime('%Y-%m-%d')} to {multivar_data[region].index[-1].strftime('%Y-%m-%d')}\"\n",
    "    )\n",
    "\n",
    "# And winsorized data\n",
    "multivar_data_win = {}\n",
    "for region, columns in variable_sets_t.items():\n",
    "    multivar_data_win[region] = preprocess_multivar(datasets_win[region], columns)\n",
    "    print(\n",
    "        f\"{region.upper()} winsorized: {multivar_data_win[region].shape} - columns: {list(multivar_data_win[region].columns)}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"{region.upper()} winsorized time series range: {multivar_data_win[region].index[0].strftime('%Y-%m-%d')} to {multivar_data_win[region].index[-1].strftime('%Y-%m-%d')}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae8c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Scree Plots and Data Storage ---\n",
    "regions = ['ch', 'eu', 'us']\n",
    "split_percentages = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "variance_thresholds = {\n",
    "    '50%': 0.50,\n",
    "    '75%': 0.75,\n",
    "    '80%': 0.80,\n",
    "    '90%': 0.90\n",
    "}\n",
    "threshold_colors = {'50%': 'green', '75%': 'orange', '80%': 'blue', '90%': 'red'}\n",
    "\n",
    "# Initialize a list to store the results for the DataFrame\n",
    "results_rows = []\n",
    "\n",
    "# --- Main Loop to Generate Plots and Collect Data ---\n",
    "for region in regions:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    fig.suptitle(f'Scree Plots for {region.upper()} Region by Training Split', fontsize=18)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    data_matrix = multivar_data_win[region].copy()\n",
    "    data_matrix = data_matrix.select_dtypes(include=[np.number]).dropna(axis=1, how='all')\n",
    "    T_total, N_total = data_matrix.shape\n",
    "\n",
    "    for i, sp in enumerate(split_percentages):\n",
    "        ax = axes[i]\n",
    "        split_size = int(T_total * sp)\n",
    "        Y_train = data_matrix.iloc[:split_size, :].dropna(axis=0)\n",
    "        \n",
    "        if Y_train.shape[0] < 2 or Y_train.shape[1] < 2:\n",
    "            ax.set_title(f\"Split {sp*100:.0f}%: Not enough data\")\n",
    "            continue\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        Y_scaled = scaler.fit_transform(Y_train)\n",
    "        pca = PCA(n_components=Y_scaled.shape[1])\n",
    "        pca.fit(Y_scaled)\n",
    "        cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "        ax.plot(range(1, len(cumulative_variance) + 1), pca.explained_variance_ratio_, 'o-', label='Individual Variance')\n",
    "        ax.set_title(f'Split: {sp*100:.0f}% (T={Y_train.shape[0]}, N={Y_train.shape[1]})')\n",
    "        ax.set_xlabel('Number of Factors')\n",
    "        ax.set_ylabel('Proportion of Variance Explained')\n",
    "        ax.grid(True)\n",
    "        ax.set_xticks(range(1, len(cumulative_variance) + 1, 2))\n",
    "        ax.set_xticklabels(range(1, len(cumulative_variance) + 1, 2))\n",
    "\n",
    "        for label, threshold in variance_thresholds.items():\n",
    "            num_factors_needed = np.argmax(cumulative_variance >= threshold) + 1\n",
    "            results_rows.append({\n",
    "                \"region\": region, \"split\": sp, \"threshold\": label, \"num_factors_needed\": num_factors_needed\n",
    "            })\n",
    "            if num_factors_needed > 0:\n",
    "                ax.axvline(x=num_factors_needed, color=threshold_colors[label], linestyle='--', \n",
    "                           label=f'{label} Var. ({num_factors_needed} factors)')\n",
    "        ax.legend()\n",
    "\n",
    "    for j in range(len(split_percentages), len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# --- Display Detailed Results per Region ---\n",
    "print(\"\\n--- Number of Factors Needed per Split ---\")\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "nf_analysis_df = pd.DataFrame(results_rows)\n",
    "\n",
    "# Loop through each region and create a pivot table for it\n",
    "for region in regions:\n",
    "    print(f\"\\n--- {region.upper()} ---\")\n",
    "    \n",
    "    # Filter the DataFrame for the current region\n",
    "    region_df = nf_analysis_df[nf_analysis_df['region'] == region]\n",
    "    \n",
    "    # Create the pivot table with splits as rows and thresholds as columns\n",
    "    split_pivot = region_df.pivot_table(\n",
    "        index='split', \n",
    "        columns='threshold', \n",
    "        values='num_factors_needed'\n",
    "    )\n",
    "    \n",
    "    # Reorder columns for logical presentation\n",
    "    split_pivot = split_pivot[['50%', '75%', '80%', '90%']]\n",
    "    \n",
    "    print(split_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609ff2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create the Multi-Index Lookup Series for NF ---\n",
    "try:\n",
    "    nf_lookup = nf_analysis_df.set_index(['region', 'split', 'threshold'])['num_factors_needed']\n",
    "    variance_thresholds = nf_analysis_df['threshold'].unique()\n",
    "    print(\"Successfully created NF lookup table from your analysis.\")\n",
    "    print(f\"Testing for thresholds: {list(variance_thresholds)}\")\n",
    "except NameError:\n",
    "    print(\"ERROR: The 'nf_analysis_df' DataFrame was not found.\")\n",
    "    print(\"Please run the previous cell to generate the scree plot data first.\")\n",
    "    exit()\n",
    "\n",
    "# --- Run the Full Sensitivity Analysis for Dynamic Factors ---\n",
    "regions = ['ch', 'eu', 'us']\n",
    "split_percentages = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "p_lags = [1, 2, 3]\n",
    "results_rows = []\n",
    "\n",
    "# Set up the progress bar with the total number of iterations\n",
    "total_iterations = len(regions) * len(split_percentages) * len(variance_thresholds) * len(p_lags)\n",
    "with tqdm(total=total_iterations, desc=\"Full Analysis\") as pbar:\n",
    "    for region in regions:\n",
    "        data_matrix = multivar_data_win[region].copy()\n",
    "        data_matrix = data_matrix.select_dtypes(include=[np.number]).dropna(axis=1, how='all')\n",
    "        T_total = data_matrix.shape[0]\n",
    "\n",
    "        for sp in split_percentages:\n",
    "            split_size = int(T_total * sp)\n",
    "            Y_train = data_matrix.iloc[:split_size, :].dropna(axis=0)\n",
    "            T, N = Y_train.shape\n",
    "            \n",
    "            if N < 2:\n",
    "                pbar.update(len(variance_thresholds) * len(p_lags))\n",
    "                continue\n",
    "\n",
    "            # Loop over all the variance thresholds\n",
    "            for threshold in variance_thresholds:\n",
    "                \n",
    "                # Look up the correct NF for this specific combination\n",
    "                try:\n",
    "                    NF = nf_lookup.loc[(region, sp, threshold)]\n",
    "                except KeyError:\n",
    "                    pbar.update(len(p_lags))\n",
    "                    continue # Skip if this combo doesn't exist\n",
    "\n",
    "                # Create the scaler\n",
    "                scaler = StandardScaler(with_mean=True, with_std=False).fit(Y_train.values)\n",
    "                \n",
    "                # Loop over VAR lags p\n",
    "                for p in p_lags:\n",
    "                    if T <= max(NF, p) + 5: # Ensure T is large enough for NF factors and p lags\n",
    "                        pbar.update(1)\n",
    "                        continue\n",
    "                    \n",
    "                    # Determine dynamic factors with the specific NF\n",
    "                    q1, q2 = dynamic_factors(Y_train.values, NF=NF, p=p, scaler=scaler)\n",
    "                    q = min(q1, q2)\n",
    "\n",
    "                    results_rows.append({\n",
    "                        \"region\": region,\n",
    "                        \"split\": sp,\n",
    "                        \"threshold\": threshold, # Store which threshold was used\n",
    "                        \"NF_used\": NF,\n",
    "                        \"p\": p,\n",
    "                        \"q_chosen\": q,\n",
    "                    })\n",
    "                    pbar.update(1)\n",
    "\n",
    "# --- Analyze and Display the Comprehensive Results ---\n",
    "dynamic_ng_df_full_analysis = pd.DataFrame(results_rows)\n",
    "\n",
    "print(\"\\n--- Sensitivity Analysis Results ---\")\n",
    "\n",
    "for r in dynamic_ng_df_full_analysis[\"region\"].unique():\n",
    "    print(f\"\\n--- Region: {r.upper()} ---\")\n",
    "    \n",
    "    region_df = dynamic_ng_df_full_analysis[dynamic_ng_df_full_analysis.region == r]\n",
    "    \n",
    "    # Create a pivot table showing q_chosen for each scenario\n",
    "    final_pivot = region_df.pivot_table(\n",
    "        index=['split', 'threshold', 'NF_used'], \n",
    "        columns='p', \n",
    "        values='q_chosen'\n",
    "    )\n",
    "    print(final_pivot)\n",
    "\n",
    "# Save the results to be used in the model\n",
    "TARGET_THRESHOLD = '75%'\n",
    "TARGET_P_LAG = 1\n",
    "\n",
    "# Filter the full analysis DataFrame\n",
    "factor_selection = dynamic_ng_df_full_analysis[\n",
    "    (dynamic_ng_df_full_analysis['threshold'] == TARGET_THRESHOLD) &\n",
    "    (dynamic_ng_df_full_analysis['p'] == TARGET_P_LAG)\n",
    "].copy()\n",
    "\n",
    "# Select and rename the final columns\n",
    "final_columns = {\n",
    "    'region': 'region',\n",
    "    'split': 'split',\n",
    "    'NF_used': 'static_factors',\n",
    "    'q_chosen': 'dynamic_factors'\n",
    "}\n",
    "factor_selection = factor_selection[final_columns.keys()].rename(columns=final_columns)\n",
    "\n",
    "print(f\"\\n Optimal factors for threshold {TARGET_THRESHOLD} and p_lag {TARGET_P_LAG} using Scree Plots:\")\n",
    "print(factor_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0bbd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bai & Ng (2002) Static Factor Analysis ---\n",
    "regions = ['ch', 'eu', 'us']\n",
    "split_percentages = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "results_rows_static = []\n",
    "\n",
    "for region in regions:\n",
    "    data_matrix = multivar_data_win[region].select_dtypes(include=[np.number]).dropna(axis=1, how='all')\n",
    "    T_total, N_total = data_matrix.shape\n",
    "\n",
    "    for sp in split_percentages:\n",
    "        split_size = int(T_total * sp)\n",
    "        Y_train = data_matrix.iloc[:split_size, :].dropna(axis=0)\n",
    "\n",
    "        if Y_train.shape[0] < 2 or Y_train.shape[1] < 2:\n",
    "            continue\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        Y_scaled = scaler.fit_transform(Y_train)\n",
    "        \n",
    "        # Calculate optimal number of factors using Bai & Ng criteria\n",
    "        _, optimal_factors = bai_ng_criteria(Y_scaled, max_factors=Y_scaled.shape[1] - 1)\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            \"region\": region,\n",
    "            \"split\": sp,\n",
    "            \"T\": Y_train.shape[0],\n",
    "            \"N\": Y_train.shape[1],\n",
    "        }\n",
    "        result.update(optimal_factors)\n",
    "        results_rows_static.append(result)\n",
    "\n",
    "# Convert results to a DataFrame for analysis and lookup\n",
    "bn_analysis_df = pd.DataFrame(results_rows_static).set_index(['region', 'split'])\n",
    "print(\"\\n--- Optimal Static Factors (k) from Bai & Ng (2002) ---\")\n",
    "print(bn_analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4a9a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Onatski (2010) Static Factor Analysis ---\n",
    "regions = ['ch', 'eu', 'us']\n",
    "split_percentages = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "onatski_results_rows = []\n",
    "\n",
    "for region in regions:\n",
    "    data_matrix = multivar_data_win[region].select_dtypes(include=[np.number]).dropna(axis=1, how='all')\n",
    "    T_total, N_total = data_matrix.shape\n",
    "\n",
    "    for sp in split_percentages:\n",
    "        split_size = int(T_total * sp)\n",
    "        Y_train = data_matrix.iloc[:split_size, :].dropna(axis=0)\n",
    "\n",
    "        if Y_train.shape[0] < 2 or Y_train.shape[1] < 2:\n",
    "            continue\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        Y_scaled = scaler.fit_transform(Y_train)\n",
    "        \n",
    "        num_factors_onatski = np.nan # Default to NaN\n",
    "        \n",
    "        try:\n",
    "            # Perform PCA to get the eigenvalues\n",
    "            # The number of components must be at least N to get all eigenvalues\n",
    "            n_components = Y_scaled.shape[1]\n",
    "            pca = PCA(n_components=n_components)\n",
    "            pca.fit(Y_scaled)\n",
    "            eigenvalues = pca.explained_variance_\n",
    "            \n",
    "            # Apply the Onatski criterion\n",
    "            num_factors_onatski = onatski_criterion(eigenvalues, n_max=8)\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"Skipping Onatski for {region.upper()} split {sp}: {e}\")\n",
    "\n",
    "        # Store results\n",
    "        result = {\n",
    "            \"region\": region,\n",
    "            \"split\": sp,\n",
    "            \"T\": Y_train.shape[0],\n",
    "            \"N\": Y_train.shape[1],\n",
    "            \"onatski_factors\": num_factors_onatski\n",
    "        }\n",
    "        onatski_results_rows.append(result)\n",
    "\n",
    "# Convert results to a DataFrame for analysis and lookup\n",
    "onatski_analysis_df = pd.DataFrame(onatski_results_rows).set_index(['region', 'split'])\n",
    "\n",
    "print(\"\\n--- Optimal Static Factors (k) from Onatski (2010) ---\")\n",
    "print(onatski_analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa057904",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Running Sensitivity Analysis for Dynamic Factors (q) based on static factors from Onatski method  ---\")\n",
    "\n",
    "# Define parameters for the analysis\n",
    "regions = ['ch', 'eu', 'us']\n",
    "split_percentages = [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "p_lags = [1, 2, 3] # VAR lags to test\n",
    "results_rows_dynamic = []\n",
    "\n",
    "for region in regions:\n",
    "    data_matrix = multivar_data_win[region].select_dtypes(include=[np.number]).dropna(axis=1, how='all')\n",
    "    T_total = data_matrix.shape[0]\n",
    "\n",
    "    for sp in split_percentages:\n",
    "        split_size = int(T_total * sp)\n",
    "        Y_train = data_matrix.iloc[:split_size, :].dropna(axis=0)\n",
    "        T, N = Y_train.shape\n",
    "        \n",
    "        if N < 2:\n",
    "            continue\n",
    "\n",
    "        # Look up the number of static factors from the Onatski results\n",
    "        try:\n",
    "            NF = int(onatski_analysis_df.loc[(region, sp), 'onatski_factors'])\n",
    "        except (KeyError, ValueError):\n",
    "            print(f\"Warning: Could not find or use Onatski result for {region.upper()} split {sp}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # If NF is NaN or less than 1, skip\n",
    "        if pd.isna(NF) or NF < 1:\n",
    "            continue\n",
    "            \n",
    "        # Pre-fit the scaler for efficiency\n",
    "        scaler = StandardScaler(with_mean=True, with_std=False).fit(Y_train.values)\n",
    "        \n",
    "        # Loop over VAR lags p\n",
    "        for p in p_lags:\n",
    "            if T <= max(NF, p) + 5:  # Ensure T is large enough for the VAR model\n",
    "                continue\n",
    "            \n",
    "            # Determine dynamic factors with the specific NF from Onatski\n",
    "            q1, q2 = dynamic_factors(Y_train.values, NF=NF, p=p, scaler=scaler)\n",
    "            q = min(q1, q2)\n",
    "\n",
    "            results_rows_dynamic.append({\n",
    "                \"region\": region,\n",
    "                \"split\": sp,\n",
    "                \"k_static_factors\": NF,\n",
    "                \"p_lag\": p,\n",
    "                \"q_dynamic_factors\": q,\n",
    "            })\n",
    "\n",
    "# --- Analyze and Display the Results ---\n",
    "dynamic_analysis_df = pd.DataFrame(results_rows_dynamic)\n",
    "\n",
    "print(\"\\n--- Full Sensitivity Analysis Results ---\")\n",
    "\n",
    "for r in dynamic_analysis_df[\"region\"].unique():\n",
    "    print(f\"\\n--- Region: {r.upper()} ---\")\n",
    "    \n",
    "    region_df = dynamic_analysis_df[dynamic_analysis_df.region == r]\n",
    "    \n",
    "    # Pivot to show how q changes with the choice of p_lag\n",
    "    final_pivot = region_df.pivot_table(\n",
    "        index=['split', 'k_static_factors'], \n",
    "        columns='p_lag', \n",
    "        values='q_dynamic_factors'\n",
    "    )\n",
    "    print(final_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b8d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save the Final Selection ---\n",
    "TARGET_P_LAG = 1\n",
    "print(f\"\\n--- Generating Final Selection File for p_lag = {TARGET_P_LAG} ---\")\n",
    "\n",
    "factor_selection = dynamic_analysis_df[\n",
    "    dynamic_analysis_df['p_lag'] == TARGET_P_LAG\n",
    "].copy()\n",
    "\n",
    "final_columns = {\n",
    "    'region': 'region',\n",
    "    'split': 'split',\n",
    "    'k_static_factors': 'static_factors',\n",
    "    'q_dynamic_factors': 'dynamic_factors'\n",
    "}\n",
    "factor_selection = factor_selection[final_columns.keys()].rename(columns=final_columns)\n",
    "\n",
    "out_path_final = ROOT_DIR / \"results\" / \"tables\" / \"factor_selection.csv\"\n",
    "factor_selection.to_csv(out_path_final, index=False)\n",
    "print(f\"\\nSaved final factor selection to: '{out_path_final}'\")\n",
    "print(factor_selection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
